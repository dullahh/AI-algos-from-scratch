{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37fff0a6",
   "metadata": {},
   "source": [
    "# Mini-Batch Gradient Descent (MBGD)\n",
    "\n",
    "A variant of Stochastic Gradient Descent where a random batch of points get selectd rather than a single one for gradient computation.\n",
    "\n",
    "This is a more generalised version of gradient descent, leveraging matrices rather than just with fixed degree polynomial versions such as the univariate variant with the regular SGD implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "fdaab1b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def numpyMBGD(X, y, learning_rate=0.01, batch_size=16, epochs=100):\n",
    "    m, n = X.shape\n",
    "    theta = np.random.randn(n, 1)  # random initialization\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        shuffled_indices = np.random.permutation(m)\n",
    "        X_shuffled = X[shuffled_indices]\n",
    "        y_shuffled = y[shuffled_indices]\n",
    "\n",
    "        for i in range(0, m, batch_size):\n",
    "            xi = X_shuffled[i:i + batch_size]\n",
    "            yi = y_shuffled[i:i + batch_size]\n",
    "\n",
    "            gradients = 2 / batch_size * xi.T.dot(xi.dot(theta) - yi)\n",
    "            theta = theta - learning_rate * gradients\n",
    "\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e39a32e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 1.0469046993669393\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# Apply function to some data, in this case, 100 triples from an i.i.d from 0 to 1\n",
    "X = np.random.rand(100, 3)\n",
    "y = 5 * X[:, 0] - 3 * X[:, 1] + 2 * X[:, 2] + np.random.randn(100, 1)  # sample linear regression problem\n",
    "theta = numpyMBGD(X, y)\n",
    "\n",
    "# Predict and calculate MAE\n",
    "predictions = X.dot(theta)\n",
    "mae = mean_absolute_error(y, predictions)\n",
    "print(f\"MAE: {mae}\")  # MAE: 1.0887166179544072\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "388ad2da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import lax\n",
    "\n",
    "@jax.jit\n",
    "def jaxMBGD(X, y, learning_rate=0.01, batch_size=16, epochs=100, seed=42):\n",
    "    X = jnp.asarray(X)\n",
    "    y = jnp.asarray(y)\n",
    "    m, n = X.shape\n",
    "\n",
    "    key = jax.random.PRNGKey(seed)\n",
    "    key, subkey = jax.random.split(key)\n",
    "    theta = jax.random.normal(subkey, (n, 1))\n",
    "\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        key, subkey = jax.random.split(key)\n",
    "\n",
    "        shuffled_indices = jax.random.permutation(subkey, m)\n",
    "\n",
    "        X_shuffled = X[shuffled_indices]\n",
    "        y_shuffled = y[shuffled_indices]\n",
    "\n",
    "        for i in range(0, m, batch_size):\n",
    "            xi = X_shuffled[i:i + batch_size]\n",
    "            yi = y_shuffled[i:i + batch_size]\n",
    "\n",
    "            gradients = 2 / xi.shape[0] * xi.T.dot(xi.dot(theta) - yi)\n",
    "            theta = theta - learning_rate * gradients\n",
    "\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a4189c8",
   "metadata": {},
   "source": [
    "### Lax-scan variant\n",
    "\n",
    "This uses the jax lax scan for iterations rather than for loops, but still logically the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "bc7f3ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def MBGD_train(X, Y, epochs=10000, learningRate=0.01, seed=42, batch_size=16):\n",
    "\n",
    "    X = jnp.asarray(X)\n",
    "    y = jnp.asarray(Y)\n",
    "    m, n = X.shape\n",
    "\n",
    "    key = jax.random.PRNGKey(seed)\n",
    "    key, subkey = jax.random.split(key)\n",
    "    theta = jax.random.normal(subkey, (n, 1))\n",
    "\n",
    "\n",
    "    def MBSGD_step(carry, _):\n",
    "        theta, key = carry\n",
    "\n",
    "        key, subkey = jax.random.split(key)\n",
    "        idx = jax.random.randint(subkey, (batch_size,), 0, m)\n",
    "\n",
    "        x = X[idx]\n",
    "        y = Y[idx]\n",
    "\n",
    "\n",
    "        gradients = 2 / x.shape[0] * x.T.dot(x.dot(theta) - y)\n",
    "        theta = theta - learningRate * gradients\n",
    "\n",
    "\n",
    "        return (theta, key), None\n",
    "\n",
    "        \n",
    "\n",
    "    (theta, key), _ = lax.scan(\n",
    "        MBSGD_step,\n",
    "        (theta, key),\n",
    "        None,\n",
    "        length=epochs\n",
    "    )\n",
    "\n",
    "    return theta\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "5c4d50a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learned:  [[ 4.95552  ]\n",
      " [-3.2513397]\n",
      " [ 2.0077748]]\n"
     ]
    }
   ],
   "source": [
    "key = jax.random.PRNGKey(42)\n",
    "key, kX, kN = jax.random.split(key, 3)\n",
    "\n",
    "X = jax.random.normal(kX, (100, 3))\n",
    "noise = jax.random.normal(kN, (100, 1))\n",
    "\n",
    "y = (5 * X[:, [0]] - 3 * X[:, [1]] + 2 * X[:, [2]]) + noise\n",
    "\n",
    "theta = MBGD_train(X, y)\n",
    "\n",
    "print(\"Learned: \", theta)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
